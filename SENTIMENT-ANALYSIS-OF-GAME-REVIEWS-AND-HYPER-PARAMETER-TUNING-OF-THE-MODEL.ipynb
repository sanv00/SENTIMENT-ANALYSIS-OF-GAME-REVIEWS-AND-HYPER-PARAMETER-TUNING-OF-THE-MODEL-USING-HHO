{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4005,
     "status": "ok",
     "timestamp": 1586270405362,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "XjYmRLn7pKnK",
    "outputId": "91abd27f-5447-4f85-f506-685889a597e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Import Keras libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.optimizers import adam\n",
    "# Use TensorFlow as the backend for Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14615,
     "status": "ok",
     "timestamp": 1586270453456,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "msQYFqITpgDq",
    "outputId": "593e8fa1-b0b5-4f77-befd-faee8c6b96f9",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/gdrive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Import Google Colab drive module (if necessary)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2ZUOWOTlxlu"
   },
   "outputs": [],
   "source": [
    "# Load data from 'Train.csv' file stored in Google Drive\n",
    "root_path = 'gdrive/My Drive/Train.csv'\n",
    "data=pd.read_csv(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qw-ETLZMeQOH"
   },
   "outputs": [],
   "source": [
    "# Extract text column from data\n",
    "text = data['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38314,
     "status": "ok",
     "timestamp": 1584413098632,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "xOJjSp45-VWa",
    "outputId": "1946fc25-070e-4f05-f980-08e27392bea0"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store the count of each word in the text\n",
    "word_list ={}\n",
    "for i in tqdm(text):\n",
    "  for j in i.split():\n",
    "    if j in word_list:\n",
    "      word_list[j] +=1\n",
    "    else:\n",
    "      word_list[j]=0\n",
    "\n",
    "# Create a dictionary to store the index number of each word in the text\n",
    "word_index={}\n",
    "number=4\n",
    "for i in tqdm(text):\n",
    "  for j in i.split():\n",
    "    if  word_list[j] > 100:\n",
    "      if j not in word_index:\n",
    "        word_index[j]= number\n",
    "        number+=1\n",
    "\n",
    "print(\"Number of unique words in the dictionary : \",len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HFAywURqJXe"
   },
   "outputs": [],
   "source": [
    "# Shift all index numbers by 3 to reserve the first 3 indices\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "\n",
    "# Add special indices for padding, start, unknown, and unused words\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# Create a dictionary to map index numbers back to their corresponding words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Function to encode a review\n",
    "def encode_review(text):\n",
    "  sent=[word_index[\"<START>\"]]\n",
    "  for i in text.split():\n",
    "    if i in word_index:\n",
    "      sent.append(word_index[i])\n",
    "    else:\n",
    "      sent.append(word_index['<UNUSED>'])\n",
    "  return sent\n",
    "\n",
    "# Function to decode a review\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38306,
     "status": "ok",
     "timestamp": 1584413098633,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "h2F37oiIFGfN",
    "outputId": "928b6ec9-493b-4ea8-e587-5d2790353975"
   },
   "outputs": [],
   "source": [
    "encode_review('I love this movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38300,
     "status": "ok",
     "timestamp": 1584413098634,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "gDITUX4OFMSL",
    "outputId": "6dac2f81-0383-4138-f784-04eee8b727b5"
   },
   "outputs": [],
   "source": [
    "decode_review([1,13,595,104,48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41152,
     "status": "ok",
     "timestamp": 1584413101579,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "Hz_lQVNCqPUp",
    "outputId": "8b9338cb-2a88-46f1-ca7c-9c5b1d3df471"
   },
   "outputs": [],
   "source": [
    "# Encode all text data in the dataset\n",
    "train_x=[]\n",
    "for i in tqdm(text):\n",
    "  train_x.append(encode_review(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42009,
     "status": "ok",
     "timestamp": 1584413102449,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "ZAt0lp0FqV-z",
    "outputId": "bb8d6405-04ba-4717-d003-75d364d986f6"
   },
   "outputs": [],
   "source": [
    "# Pad the sequences of integers to have the same length\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_x,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42001,
     "status": "ok",
     "timestamp": 1584413102450,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "2ZnjntVfqvH4",
    "outputId": "67a3d964-24c8-4f46-ab0b-5031e1120b8c"
   },
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add an embedding layer\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "\n",
    "# Add a global average pooling layer\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "# Add dropout regularization to prevent overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add two dense layers with ReLU activation\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(32,activation='relu'))\n",
    "\n",
    "# Add another dropout layer\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add another dense layer with ReLU activation\n",
    "model.add(keras.layers.Dense(16,activation='relu'))\n",
    "\n",
    "# Add the final dense layer with sigmoid activation\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# The Embedding layer maps the input data (vocabulary words) to a lower-dimensional space. \n",
    "# The GlobalAveragePooling1D layer then reduces the dimensionality of the input data by taking the average of all the values along the time dimension.\n",
    "# The Dropout layers are applied to the output of the previous layers to prevent overfitting. \n",
    "# The Dense layers then process the input data and apply the ReLU or sigmoid activation functions. The final output layer produces the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41989,
     "status": "ok",
     "timestamp": 1584413102450,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "brniEPB1q8ov",
    "outputId": "a26eb072-9c5d-4faf-deb5-00f23c619340"
   },
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimizer, binary crossentropy loss, and accuracy metric\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxf39_FDrJuX"
   },
   "outputs": [],
   "source": [
    "# Store the labels in a separate variable\n",
    "train_y = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115401,
     "status": "ok",
     "timestamp": 1584413175876,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "I2WZDYmErPZk",
    "outputId": "38f06ffa-b49a-41ac-8655-4ed759386bd8"
   },
   "outputs": [],
   "source": [
    "# Fit the model on the training data with validation split of 0.2, 50 epochs, and batch size of 512\n",
    "history = model.fit(train_data,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    validation_split=0.2,\n",
    "                    batch_size=512,\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115391,
     "status": "ok",
     "timestamp": 1584413175877,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "qdCnNC9Zsrov",
    "outputId": "37fc732b-e1e5-4a85-afcb-9855f332fca1"
   },
   "outputs": [],
   "source": [
    "# Test the encode and decode functions\n",
    "print(encode_review('hello there'))\n",
    "print(decode_review(encode_review(\"hello there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115386,
     "status": "ok",
     "timestamp": 1584413175878,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "D_qRRGmNsxhH",
    "outputId": "f2bb5c40-46f1-4b4d-a777-04e0d9be1d2a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Pad the review list using the pad_sequences function\n",
    "# The value used for padding is the word index for the \"<PAD>\" token\n",
    "# Padding is added to the end of the sequences (post-padding)\n",
    "# The maximum length of the padded sequences is 256\n",
    "emb = keras.preprocessing.sequence.pad_sequences([encode_review('Though it was bad we enjoyed the movie and we liked it')],\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)\n",
    "\n",
    "# Use the model to predict the sentiment of the review.\n",
    "model.predict(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-XQUWm6tIrJ"
   },
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "  emb = keras.preprocessing.sequence.pad_sequences([encode_review(text)],\n",
    "                                                  value=word_index['<PAD>'],\n",
    "                                                  padding='post',\n",
    "                                                  maxlen=256)\n",
    "  \n",
    "  pred = model.predict(emb)\n",
    "  \n",
    "  # Check the prediction and return 'positive' or 'negative'  \n",
    "  if (pred*100) >50.0 :\n",
    "    return \"positive\"\n",
    "  else:\n",
    "    return \"negative\"\n",
    " \n",
    " # Test the classify function with a negative review\n",
    "classify(\"I dont like this game\")\n",
    "\n",
    "# Test the classify function with a positive review\n",
    "classify('Though it was bad we enjoyed the movie and we liked it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115363,
     "status": "ok",
     "timestamp": 1584413175880,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "FB4IfxNavcxy",
    "outputId": "120e1596-dde5-4a58-b484-c079e454d67d"
   },
   "outputs": [],
   "source": [
    "# Get the dictionary containing the model's training history\n",
    "history_dict = history.history\n",
    "\n",
    "# Extract the training and validation accuracy and loss values\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "# Get the number of epochs\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# Plot the training loss values\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "\n",
    "# Plot the validation loss values\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "\n",
    "# Set the plot title and labels\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115356,
     "status": "ok",
     "timestamp": 1584413175881,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "SAZjRxQkx-D0",
    "outputId": "16d5d670-3776-4934-bcbc-4671d45079bc"
   },
   "outputs": [],
   "source": [
    "# Plot the training accuracy values\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "\n",
    "# Plot the validation accuracy values\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "\n",
    "# Set the plot title and labels\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115349,
     "status": "ok",
     "timestamp": 1584413175881,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "BE9wmZjLyKmd",
    "outputId": "740314d7-5155-4e3f-cb20-e1b7b4dce8a9"
   },
   "outputs": [],
   "source": [
    "# Load data from 'steam_reviews.csv' file stored in Google Drive\n",
    "path='gdrive/My Drive/steam_reviews.csv'\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aA7FznNPrfJj"
   },
   "outputs": [],
   "source": [
    "# Get the values from the 'review' and 'helpful' columns\n",
    "reviews = data['review'].values\n",
    "helpful = data['helpful'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115330,
     "status": "ok",
     "timestamp": 1584413175882,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "R02nBvklsHys",
    "outputId": "0a519d1f-6de1-4cd3-8291-2d439adaf242"
   },
   "outputs": [],
   "source": [
    "# Initialize empty lists for storing the labeled sentiment of each review, \n",
    "# and the indices of the reviews that were successfully labeled\n",
    "labeled_sentiment = []\n",
    "idx =[]\n",
    "\n",
    "# Iterate through the reviews and classify the sentiment of each review\n",
    "for index , i in tqdm(enumerate(reviews[:10000])):\n",
    "  try :\n",
    "    labeled_sentiment.append(classify(i))\n",
    "    idx.append(index)\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115323,
     "status": "ok",
     "timestamp": 1584413175883,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "EHVzX0ArsqA2",
    "outputId": "8134ff76-e5dc-4c47-c0c3-124fa3292688"
   },
   "outputs": [],
   "source": [
    "# Initialize empty lists for storing the final reviews and helpful reviews\n",
    "final_reviews=[]\n",
    "final_helpful=[]\n",
    "for i in tqdm(range(len(reviews[:10000]))):\n",
    "  if i in idx:\n",
    "    final_reviews.append(reviews[i])\n",
    "    final_helpful.append(helpful[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dyLUd7sstiJm"
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe using the filtered reviews and helpfulness ratings, \n",
    "# and the labeled sentiment of each review\n",
    "new_data = pd.DataFrame(data=list(zip(final_helpful,final_reviews,labeled_sentiment)),columns=['helpful','review','sentiment'])\n",
    "\n",
    "# Get the values from the 'review' and 'sentiment' columns in the new dataframe\n",
    "reviews = new_data['review'].values\n",
    "label = new_data['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWIcmoN7xWwX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115311,
     "status": "ok",
     "timestamp": 1584413175884,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "OnNNZJ2gxoxu",
    "outputId": "12a0fa50-f8f9-4b51-f682-63c71ed28a97"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary for storing the mapping of words to indices\n",
    "word_dict={}\n",
    "val=1\n",
    "for i in tqdm(reviews):\n",
    "  for j in i.split():\n",
    "    if j not in word_dict:\n",
    "      word_dict[j]=val\n",
    "      val+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLeGbgMqzsZ9"
   },
   "outputs": [],
   "source": [
    "# Add special tokens to the word dictionary\n",
    "word_dict['<PAD>']=0\n",
    "word_dict['<START>']=val\n",
    "word_dict['<UNK>'] = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115302,
     "status": "ok",
     "timestamp": 1584413175884,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "otBGQBccz8Fx",
    "outputId": "e8128354-febc-496d-f90b-4fa514bcf73c"
   },
   "outputs": [],
   "source": [
    "# Convert the reviews to a list of lists of integers\n",
    "train_data =[]\n",
    "for i in tqdm(reviews):\n",
    "  sent = [word_dict['<START>']]\n",
    "  for j in i.split():\n",
    "    sent.append(word_dict[j])\n",
    "  train_data.append(sent)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3J-XhB61IE0"
   },
   "outputs": [],
   "source": [
    "# Pad the reviews so that they all have the same length\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_dict[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80TmIkFv2vGE"
   },
   "outputs": [],
   "source": [
    "# Convert labels to a list of integers (0 for negative, 1 for positive)\n",
    "train_out =[]\n",
    "for i in label:\n",
    "  if i == 'negative':\n",
    "    train_out.append(0)\n",
    "  else:\n",
    "    train_out.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJqNNO5H_HDX"
   },
   "source": [
    "The neural_network() method is the base architechture that we are going to use to train our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQN1P9Xe1jYJ"
   },
   "outputs": [],
   "source": [
    "def neural_network(drop=0.2):\n",
    "    # Create a Sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an Embedding layer with vocabulary size and embedding dimension\n",
    "    model.add(keras.layers.Embedding(len(word_dict), 16))\n",
    "    \n",
    "    # Add a GlobalAveragePooling1D layer to average the embeddings\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "\n",
    "    # Add a Dense layer with 32 units and ReLU activation\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "    # Add a Dense layer with 64 units and ReLU activation\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    # Add a Dropout layer with the specified dropout rate\n",
    "    model.add(keras.layers.Dropout(drop))\n",
    "\n",
    "    # Add a Dense layer with 1 unit and sigmoid activation\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhNWdCrO9kjF"
   },
   "source": [
    "# **Harris Hawks Optimization Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CedWqAbw16-V"
   },
   "outputs": [],
   "source": [
    "class HHO():\n",
    "  \n",
    "  # Initialize the HHO algorithm with the number of individuals in the population (N), the number of iterations to be performed (T), \n",
    "  # the number of features in the problem (nfeatures), and an optional population of individuals (pop)\n",
    "  def __init__(self,N,T,nfeatures,pop=[]):\n",
    "    self.N=N\n",
    "    self.T=T\n",
    "    self.n_features = nfeatures\n",
    "    # If a population is not provided, initialize it with random individuals\n",
    "    if len(pop)==0:\n",
    "      self.population = np.random.random((self.N,self.n_features))\n",
    "    # If a population is provided, use it\n",
    "    else:\n",
    "      self.population=pop[:]\n",
    "    self.best_fitness =0\n",
    "    \n",
    "    \n",
    "  def optimize(self):\n",
    "    # Set the upper and lower bounds for the population\n",
    "    self.UB = len(self.population) - 1\n",
    "    self.LB = 0\n",
    "    \n",
    "    # Iterate for the specified number of iterations\n",
    "    for i in tqdm(range(self.T)):\n",
    "        # If this is the first iteration, print a newline character\n",
    "        if i == 0:\n",
    "            print()\n",
    "        # Initialize an empty list to store the fitness values of the individuals\n",
    "        fits = []\n",
    "        \n",
    "        # Evaluate the fitness of each individual in the population\n",
    "        for j in range(len(self.population)):\n",
    "            # If this is not the first iteration, apply a sigmoid function to the individual's first feature\n",
    "            if i != 0:\n",
    "                indi_drop = self.sigmoid(self.population[j][0])\n",
    "            # If this is the first iteration, use the individual's first feature as is\n",
    "            else:\n",
    "                indi_drop = self.population[j][0]\n",
    "            # Create a neural network with the modified first feature\n",
    "            model = neural_network(indi_drop)\n",
    "            # Compile the model\n",
    "            model.compile(optimizer='adam', loss='mse', metrics=['acc'])\n",
    "            # Fit the model to the training data and evaluate its accuracy\n",
    "            history = model.fit(train_data, train_out, epochs=1, batch_size=8)\n",
    "            acc = history.history['acc'][0]\n",
    "            # Store the individual's accuracy as its fitness\n",
    "            fits.append(acc)\n",
    "            # Update the best fitness value if the current individual has a higher fitness\n",
    "            best_fitness = max(fits)\n",
    "            if acc >= best_fitness:\n",
    "                # Store the index of the best individual\n",
    "                index = j\n",
    "                self.best_fitness = best_fitness\n",
    "        \n",
    "        # If this is not the first iteration, use the sigmoid function to modify the best individual's first feature\n",
    "        if i != 0:\n",
    "            self.rabbit = self.population[index][0]\n",
    "        # If this is the first iteration, use the best individual's first feature as is\n",
    "        else:\n",
    "            self.rabbit = self.sigmoid(self.population[index][0])\n",
    "        # Print the selected dropout rate\n",
    "        print()\n",
    "        print(\"Dropout selected from Iteration \", i+1, \" : \", self.rabbit)\n",
    "        print()\n",
    "        \n",
    "        # Update the rest of the population\n",
    "        for t, hawk in enumerate(self.population):\n",
    "            # Generate random values for E and J\n",
    "            E = 2 * np.random.rand() - 1\n",
    "            J = 2 * (1 - np.random.rand())\n",
    "            \n",
    "            # Update the value of E based on the current iteration\n",
    "            E = self.update_E(E, i+1)\n",
    "            \n",
    "            # If E is greater than or equal to 1, perform exploration\n",
    "            if np.abs(E) >= 1:\n",
    "                self.exploration(t)\n",
    "            \n",
    "            # If E is less than 1, perform exploitation or cooperation\n",
    "            if np.abs(E) < 1:\n",
    "                # Generate a random value for r\n",
    "                r = np.random.rand()\n",
    "          \n",
    "          if r>=0.5 and np.abs(E)>=0.5 :\n",
    "            #update vector using eqn(4)\n",
    "            self.soft_baise(t,E,J)\n",
    "          elif r>=0.5 and np.abs(E) <0.5:\n",
    "            #update vector using eqn (6)\n",
    "            self.hard_baise(t,E)\n",
    "          elif r<0.5 and np.abs(E) >=0.5:\n",
    "            #update vector using eqn (10)\n",
    "            self.soft_baise_dive(t,E,J)\n",
    "          elif r<0.5 and np.abs(E) <0.5:\n",
    "            #update vector using eqn (11)\n",
    "            self.hard_baise_dive(t,E,J)\n",
    "           \n",
    "      #print(\"\\t Best fitness value in this iteration = \",best_fitness) \n",
    "      #print(\"best rabbit in this iteration : \",self.rabbit)\n",
    "    \n",
    "    return self.rabbit,index\n",
    "\n",
    "  def update_E(self,E,t):\n",
    "    # update the value of E at each time step\n",
    "    E = 2 * E *(1 - t/self.T)\n",
    "    return E\n",
    "  \n",
    "  def soft_baise(self,t,E,J):\n",
    "    # update population using soft baise approach\n",
    "    del_x = self.rabbit - self.population[t]\n",
    "    \n",
    "    # update population at time (t+1)%UB+1\n",
    "    self.population[(t+1)%self.UB+1]= del_x  - E * np.abs( J * self.rabbit - self.population[t] )\n",
    "  \n",
    "  def hard_baise(self,t,E):\n",
    "    # update population using hard baise approach\n",
    "    del_x = self.rabbit - self.population[t]\n",
    "    \n",
    "    # update population at time (t+1)%UB+1\n",
    "    self.population[(t+1)%self.UB+1] = self.rabbit - E * np.abs(del_x)\n",
    "  \n",
    "  def soft_baise_dive(self,t,E,J):\n",
    "    # update population using soft baise with dive approach\n",
    "    Y = self.rabbit - E* np.abs(J * self.rabbit - self.population[t])\n",
    "    \n",
    "    # sample a random noise vector\n",
    "    s = np.random.randn(1,self.n_features)\n",
    "    \n",
    "    u=np.random.random()\n",
    "    v= np.random.random()\n",
    "    \n",
    "    # calculate standard deviation of population at time t\n",
    "    std = np.std(self.population[t])\n",
    "    beta = 1/1.5\n",
    "    \n",
    "    # calculate the scaling factor for the noise vector\n",
    "    lf= 0.01 * u * std /(np.abs(v)**beta)\n",
    "    \n",
    "    # create a new candidate solution by adding the noise vector to Y\n",
    "    Z = Y + s * lf\n",
    "    \n",
    "    # update population at time (t+1)%UB+1 based on the fitness of Y and Z\n",
    "    if self.fitness(Y) < self.fitness(self.population[t]):\n",
    "      self.population[(t+1)%self.UB+1] = Y\n",
    "    elif self.fitness(Z) < self.fitness(self.population[t]):\n",
    "      self.population[(t+1)%self.UB+1] = Z\n",
    "  \n",
    "  def hard_baise_dive(self,t,E,J):\n",
    "    # calculate average of the population at time t\n",
    "    avg=0  \n",
    "    for i in self.population:\n",
    "      avg+=i\n",
    "  \n",
    "    pop = len(self.population)\n",
    "    avg=avg/pop\n",
    "    \n",
    "    # calculate Y using hard baise approach\n",
    "    Y = self.rabbit - E* np.abs(J*self.rabbit - avg)\n",
    "    \n",
    "    # sample a random noise vector\n",
    "    s = np.random.randn(1,self.n_features)\n",
    "    \n",
    "    u=np.random.random()\n",
    "    v= np.random.random()\n",
    "    \n",
    "    # calculate standard deviation of population at time t\n",
    "    std = np.std(self.population[t])\n",
    "    beta = 1/1.5\n",
    "    \n",
    "    # calculate the scaling factor for the noise vector\n",
    "    lf= 0.01 * u * std /(np.abs(v)**beta)\n",
    "    \n",
    "    # create a new candidate solution by adding the noise vector to Y\n",
    "    Z = Y + s * lf\n",
    "    \n",
    "    # update population at time (t+1)%UB+1 based on the fitness of Y and Z\n",
    "    if self.fitness(Y) < self.fitness(self.population[t]):\n",
    "      self.population[(t+1)%self.UB+1] = Y\n",
    "    elif self.fitness(Z) < self.fitness(self.population[t]):\n",
    "      self.population[(t+1)%self.UB+1] = Z\n",
    "  \n",
    "  def exploration(self,t):\n",
    "    # update population using exploration approach\n",
    "    q = np.random.random()\n",
    "    r1= np.random.random()\n",
    "    r2= np.random.random()\n",
    "    r3= np.random.random()\n",
    "    r4= np.random.random()\n",
    "  \n",
    "    # calculate average of the population at time t\n",
    "    avg=0  \n",
    "    for i in self.population:\n",
    "      avg+=i\n",
    "  \n",
    "    pop = len(self.population)\n",
    "    avg=avg/pop\n",
    "   \n",
    "    # select a random solution from the population\n",
    "    k = np.random.randint(low=0,high=pop-1)\n",
    "    \n",
    "    if q >= 0.5 :\n",
    "      # update population at time (t+1)%pop based on the selected solution and the current solution\n",
    "      self.population[(t+1)%pop] = self.population[k] -r1 * (self.population[k] - 2* r2 *self.population[t])\n",
    "    else :\n",
    "      # update population at time (t+1)%pop based on the average of the population and the rabbit\n",
    "      self.population[(t+1)%pop] = (self.rabbit - avg) - r3 *( 0 + r4 * (pop-1 - 0))\n",
    "    \n",
    "  def fitness(self,hawk):\n",
    "    # calculate the fitness of the given solution\n",
    "    sum=np.mean(hawk)\n",
    "    return sum\n",
    "\n",
    "  def sigmoid(self,x):\n",
    "    # calculate the sigmoid function\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bccat0Rn2tKW"
   },
   "outputs": [],
   "source": [
    "def create_population(pop_size):\n",
    "  # create a population of solutions\n",
    "  pop=[]\n",
    "  for i in range(pop_size):\n",
    "    # create an individual solution\n",
    "    individual=[]\n",
    "    # sample a random dropout rate\n",
    "    dropout = np.random.random()\n",
    "    # add the dropout rate to the individual solution\n",
    "    individual.append(dropout)\n",
    "    # add the individual solution to the population\n",
    "    pop.append(individual)\n",
    "  # return the population as a numpy array\n",
    "  return np.array(pop,dtype='float32')\n",
    "\n",
    "# create a population of size 5\n",
    "print(create_population(5))\n",
    "# create a population of size 25\n",
    "pop = create_population(25)\n",
    "print('length : ',len(pop))\n",
    "\n",
    "# create an instance of HHO with the given population\n",
    "opt = HHO(len(pop),2,1,pop)\n",
    "# optimize the population\n",
    "optimized_dropout = opt.optimize()\n",
    "\n",
    "# print the optimized dropout rate\n",
    "print(\"Optimized Dropout : \",optimized_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cHnxe1f_-Dki"
   },
   "source": [
    "#**Training Our Main Model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 874
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1151663,
     "status": "ok",
     "timestamp": 1584414212281,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "Km-6zfro2tp8",
    "outputId": "d5de448f-e126-445a-ae99-32dc2a83db32"
   },
   "outputs": [],
   "source": [
    "# create an instance of the neural_network model with dropout rate 0\n",
    "model = neural_network(0)\n",
    "\n",
    "# compile the model with mean squared error as the loss function, Adam optimizer and accuracy as the metric\n",
    "model.compile(loss='mse',optimizer='adam',metrics=['acc'])\n",
    "\n",
    "# fit the model on the training data and output, with 25 epochs and a validation split of 0.2\n",
    "h1 = model.fit(train_data,train_out,epochs=25,validation_split=0.2,batch_size=8)\n",
    "\n",
    "# create an instance of the neural_network model with dropout rate 2\n",
    "model = neural_network(2)\n",
    "\n",
    "# compile the model with mean squared error as the loss function, Adam optimizer and accuracy as the metric\n",
    "model.compile(loss='mse',optimizer='adam',metrics=['acc'])\n",
    "\n",
    "# fit the model on the training data and output, with 25 epochs and a validation split of 0.2\n",
    "h2 = model.fit(train_data,train_out,epochs=25,validation_split=0.2,batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 874
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1736376,
     "status": "ok",
     "timestamp": 1584414797003,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "NHo6hyDYAnho",
    "outputId": "d0e491a4-16d3-405a-b9ce-1b444d1c7bf1"
   },
   "source": [
    "### model = neural_network(0.271245)\n",
    "model.compile(loss='mse',optimizer='adam',metrics=['acc'])\n",
    "h3 = model.fit(train_data,train_out,epochs=25,validation_split=0.2,batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUoQS7KT3cHo"
   },
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "  out =[word_dict['<START>']]\n",
    "  for i in text.split():\n",
    "    if i in word_dict:\n",
    "      out.append(word_dict[i])\n",
    "    else:\n",
    "      out.append(word_dict['<UNK>'])\n",
    "      \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgTVbwjw4_Hl"
   },
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "  emb = keras.preprocessing.sequence.pad_sequences([encode(text)],\n",
    "                                                  value=word_dict['<PAD>'],\n",
    "                                                  padding='post',\n",
    "                                                  maxlen=256)\n",
    "  \n",
    "  pred = model.predict(emb)\n",
    "  \n",
    "  #print(\"prediction obtained : \", pred)\n",
    "  \n",
    "  if (pred*100) >50.0 :\n",
    "    #print(\"Positive review\")\n",
    "    return \"positive\"\n",
    "  else:\n",
    "    #print(\"Negative review\")\n",
    "    return \"negative\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1738650,
     "status": "ok",
     "timestamp": 1584414799293,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "A0Ff80s65Fgy",
    "outputId": "7979e473-a0f9-4eff-db8f-4562104ad9c5"
   },
   "outputs": [],
   "source": [
    "classify('bad gaME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1738642,
     "status": "ok",
     "timestamp": 1584414799294,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "ldYKK_8c0q_s",
    "outputId": "74aa574c-0c66-4fc0-c38b-5b54eab2474a"
   },
   "outputs": [],
   "source": [
    "# create a list of 25 integers\n",
    "y = list(range(1,26))\n",
    "\n",
    "# print the list\n",
    "print(y)\n",
    "# print the accuracy values for the first model\n",
    "print(h1.history['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the validation accuracy for the three models\n",
    "plt.plot(y,h1.history['val_acc'])\n",
    "plt.plot(y,h2.history['val_acc'])\n",
    "plt.plot(y,h3.history['val_acc'])\n",
    "\n",
    "# add a legend to the plot\n",
    "plt.legend(['No Dropout','Constant Dropout','With HHO'])\n",
    "\n",
    "# create a boxplot of the validation accuracy for the three models\n",
    "plt.boxplot([h1.history['val_acc'],h2.history['val_acc'],h3.history['val_acc']])\n",
    "\n",
    "# add labels to the x-axis\n",
    "plt.xlabel(['No Dropout','Constant Dropout','with HHO dropout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1738638,
     "status": "ok",
     "timestamp": 1584414799294,
     "user": {
      "displayName": "nYuker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiL6S7ymhC6slqP1-RSNF-WfqBNVR7i50GYoomBtA=s64",
      "userId": "07887260677890672048"
     },
     "user_tz": -330
    },
    "id": "G3MxBQ8C02Yh",
    "outputId": "d3044b35-8ad0-4653-bb8a-842185d64951"
   },
   "outputs": [],
   "source": [
    "# print the maximum validation accuracy for each model\n",
    "print('No Dropout       : ',max(h1.history['val_acc']))\n",
    "print('Constant Dropout : ',max(h2.history['val_acc']))\n",
    "print('with HHO dropout : ',max(h3.history['val_acc']))\n",
    "\n",
    "# print optimized dropout\n",
    "print('Optimized Dropout:',optimized_dropout)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zhNWdCrO9kjF",
    "cHnxe1f_-Dki"
   ],
   "name": "San_Vinoth_Project(1) (4).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
